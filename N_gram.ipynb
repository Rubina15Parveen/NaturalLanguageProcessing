{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data for calculations"
      ],
      "metadata": {
        "id": "dSJm6_w8I0dg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzY0YHMrIy9U",
        "outputId": "005a6001-5ce9-4024-f546-7c5dd6b177c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.lm import MLE, Laplace, StupidBackoff\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "baH5J_q5I9HH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267c9528-92c5-4b3c-c81a-fbda4509743e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Rubina15Parveen/NgramsLanguageModel/59c2ee7e5abf9606ed8ba7de92079a5bf49c3662/corpus_for_language_model.txt\""
      ],
      "metadata": {
        "id": "ilWFiieHJAzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "  corpus = response.text.splitlines()\n",
        "  print(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf8GdeRFJDs3",
        "outputId": "48bc71a7-e1b8-451b-86ad-66c86bffa203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Richard W. Lock , retired vice president and treasurer of Owens-Illinois Inc. , was named a director of this transportation industry supplier , increasing its board to six members . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [wordpunct_tokenize(sentence) for sentence in corpus]\n",
        "#tokenized_sentences[0]"
      ],
      "metadata": {
        "id": "RHPJOpCLJIBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.A : Maximum Likelihood Estimation using the trigram model"
      ],
      "metadata": {
        "id": "QHfoCwjntH8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "train_data, vocab = padded_everygram_pipeline(n, tokenized_sentences)\n",
        "model1 = MLE(n)\n",
        "model1.fit(train_data, vocab)"
      ],
      "metadata": {
        "id": "UWP7Bw6zJLjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S1 = \"Senator Byrd is chairman of the Appropriations Committee .\"\n",
        "S2 = \"His appointment expands the board to 13 members .\"\n",
        "S1_tokens = list(pad_both_ends(wordpunct_tokenize(S1), n))\n",
        "S2_tokens = list(pad_both_ends(wordpunct_tokenize(S2), n))\n",
        "print(S1_tokens)\n",
        "print(S2_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653oWbigJPr3",
        "outputId": "3d2fb999-8eba-4d5f-8796-207587c78205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'Senator', 'Byrd', 'is', 'chairman', 'of', 'the', 'Appropriations', 'Committee', '.', '</s>', '</s>']\n",
            "['<s>', '<s>', 'His', 'appointment', 'expands', 'the', 'board', 'to', '13', 'members', '.', '</s>', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probability using the MLE model rounded of to nearest integer :"
      ],
      "metadata": {
        "id": "z5H8TDy8toAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_probabilities(model, sentence):\n",
        "    log_prob = 0.0\n",
        "    for i in range(len(sentence) - n + 1):\n",
        "        ngram = tuple(sentence[i:i+n])\n",
        "        log_prob += model.logscore(ngram[-1], ngram[:-1])\n",
        "    return log_prob\n",
        "\n",
        "log_prob_S1 = log_probabilities(model1, S1_tokens)\n",
        "log_prob_S2 = log_probabilities(model1, S2_tokens)\n",
        "print(f\"Sentence 1 Probability : {log_prob_S1}\".format(log_prob_S1))\n",
        "print(f\"Sentence 2 Probability : {log_prob_S2}\".format(log_prob_S2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOtLR2OtJUBa",
        "outputId": "a4eaa540-8fe1-46b1-f038-c1d74391ba6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 Probability : -19.446068605660464\n",
            "Sentence 2 Probability : -18.383636799547986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1B: Adding one Laplace Smoothening"
      ],
      "metadata": {
        "id": "HEU2BNvtJe7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define n-gram order (trigrams) and prepare training data\n",
        "tokenized_sentences = [wordpunct_tokenize(sentence) for sentence in corpus]\n",
        "n = 3\n",
        "train_data, vocab = padded_everygram_pipeline(n, tokenized_sentences)\n",
        "\n",
        "# Initialize and fit the Laplace model (Add-One Smoothing)\n",
        "model2 = Laplace(n)\n",
        "model2.fit(train_data, vocab)\n",
        "\n",
        "# Calculate log probabilities using Laplace smoothing\n",
        "log_prob_S1_Laplace = log_probabilities(model2, S1_tokens)\n",
        "log_prob_S2_Laplace = log_probabilities(model2, S2_tokens)\n",
        "\n",
        "# Output log probabilities (rounded)\n",
        "print(f\"Laplace - Sentence 1 Log Probability: {round(log_prob_S1_Laplace)}\")\n",
        "print(f\"Laplace - Sentence 2 Log Probability: {round(log_prob_S2_Laplace)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8WUFmpWJXaE",
        "outputId": "648a8227-72ac-4525-c745-7164876791e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace - Sentence 1 Log Probability: -107\n",
            "Laplace - Sentence 2 Log Probability: -109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1B: With Katz back off smoothing"
      ],
      "metadata": {
        "id": "7PHhZ7Wx0Hjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for log probabilities using the StupidBackoff model\n",
        "tokenized_sentences = [wordpunct_tokenize(sentence) for sentence in corpus]\n",
        "n = 3\n",
        "train_data, vocab = padded_everygram_pipeline(n, tokenized_sentences)\n",
        "\n",
        "# Initialize and fit the StupidBackoff model with the n-gram order (3 in this case)\n",
        "alpha = 0.4  # Katz back-off alpha\n",
        "model3 = StupidBackoff(alpha, n)  # Specify the n-gram order\n",
        "\n",
        "# Fit the model using the same training data and vocabulary\n",
        "model3.fit(train_data, vocab)\n",
        "\n",
        "# Ensure proper fallback happens in case trigrams are not found\n",
        "log_prob_S1_backoff = log_probabilities(model3, S1_tokens)\n",
        "log_prob_S2_backoff = log_probabilities(model3, S2_tokens)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Katz Backoff - Sentence 1 Log Probability: {log_prob_S1_backoff}\")\n",
        "print(f\"Katz Backoff - Sentence 2 Log Probability: {log_prob_S2_backoff}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpn8sexo0UkO",
        "outputId": "6768ea11-9154-4846-becc-d7ee33bae5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Katz Backoff - Sentence 1 Log Probability: -19.446068605660464\n",
            "Katz Backoff - Sentence 2 Log Probability: -18.383636799547986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S3 = \"But , in this context , that 's the smart thing to do . ''\"\n",
        "S4 = \"It has n't made merger overtures to the board . \"\n",
        "S3_tokens = list(pad_both_ends(wordpunct_tokenize(S3), n))\n",
        "S4_tokens = list(pad_both_ends(wordpunct_tokenize(S4), n))\n",
        "log_prob_S3 = log_probabilities(model1, S3_tokens)\n",
        "log_prob_S4 = log_probabilities(model1, S4_tokens)\n",
        "print(f\"Sentence 1 Probability : {log_prob_S3}\".format(log_prob_S3))\n",
        "print(f\"Sentence 2 Probability : {log_prob_S4}\".format(log_prob_S4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G_fMVyXarqg",
        "outputId": "35a26db4-a64b-489b-f31f-4307b097d4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 Probability : -28.78331761074023\n",
            "Sentence 2 Probability : -22.507625516823442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S5= \"W.R. Grace holds three of Grace Energy 's seven board seats .\"\n",
        "S6= \"Mervin Lung remains chairman and chief executive officer .\"\n",
        "S5_tokens = list(pad_both_ends(wordpunct_tokenize(S5), n))\n",
        "S6_tokens = list(pad_both_ends(wordpunct_tokenize(S6), n))\n",
        "log_prob_S5 = log_probabilities(model1, S5_tokens)\n",
        "log_prob_S6 = log_probabilities(model1, S6_tokens)\n",
        "print(f\"Sentence 5 Probability : {log_prob_S5}\".format(log_prob_S5))\n",
        "print(f\"Sentence 6 Probability : {log_prob_S6}\".format(log_prob_S6))"
      ],
      "metadata": {
        "id": "qxXu3sKAbXlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d97a333-bc40-4b19-f234-dfe171b52885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 5 Probability : -27.046352016574026\n",
            "Sentence 6 Probability : -15.639639105271593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Cjc4dR1arfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Compute Positive Pointwise Mutual Information of pairs [word, context]"
      ],
      "metadata": {
        "id": "Eh1ZjTuuSGib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ppmi(tokenized_sentences):\n",
        "    # Calculate probabilities\n",
        "    word_freq = defaultdict(int)\n",
        "    context_freq = defaultdict(int)\n",
        "    pair_freq = defaultdict(int)\n",
        "    total_count = 0\n",
        "\n",
        "    #Process each sentence\n",
        "    for sentence in tokenized_sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            # Get the context window 8\n",
        "            left_context = sentence[max(0, i-8):i]\n",
        "            right_context = sentence[i+1:i+9]\n",
        "            context = ['NIL'] * (8 - len(left_context)) + left_context + right_context + ['NIL'] * (8 - len(right_context))\n",
        "\n",
        "            # Count the frequencies\n",
        "            word_freq[word] += 1\n",
        "            for c_word in context:\n",
        "                context_freq[c_word] += 1\n",
        "                pair_freq[(word, c_word)] += 1\n",
        "                total_count += 1\n",
        "    return word_freq, context_freq, pair_freq, total_count\n",
        "\n",
        "def calculate_ppmi_for_pair(target_word, context_word, word_freq, context_freq, pair_freq, total_count):\n",
        "    #word_freq, context_freq, pair_freq, total_count = calculate_ppmi(tokenized_sentences)\n",
        "    # Calculate probabilities\n",
        "    prob_word = word_freq[target_word] / total_count\n",
        "    prob_context = context_freq[context_word] / total_count\n",
        "    prob_pair = pair_freq[(target_word, context_word)] / total_count\n",
        "\n",
        "    # Calculate PMI\n",
        "    if prob_pair > 0 and prob_word > 0 and prob_context > 0:\n",
        "        pmi = math.log2(prob_pair / (prob_word * prob_context))\n",
        "        ppmi = max(0, pmi)\n",
        "        return ppmi\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Calculate frequencies once\n",
        "tokenized_sentences = [wordpunct_tokenize(sentence) for sentence in corpus]\n",
        "word_freq, context_freq, pair_freq, total_count = calculate_ppmi(tokenized_sentences)"
      ],
      "metadata": {
        "id": "3QaGclY129ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.A : PPMI of 1) executive and president 2) executive and said 3) company and of 4) company and said"
      ],
      "metadata": {
        "id": "SNESJE_5ETBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.A.1. word = executive and context word = president\n",
        "ppmi_value = calculate_ppmi_for_pair('executive', 'president', word_freq, context_freq, pair_freq, total_count)\n",
        "print(f\"PPMI for 'executive' and 'president' = {ppmi_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wOSjq4IDy3b",
        "outputId": "48739b56-7e71-4002-a09f-931d6210a539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI for 'executive' and 'president' = 6.543534603619038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.A.2. word = executive and context word = said\n",
        "target_word = 'executive'\n",
        "context_word = 'said'\n",
        "ppmi_value = calculate_ppmi_for_pair(target_word, context_word, word_freq, context_freq, pair_freq, total_count)\n",
        "print(f\"PPMI for '{target_word}' and '{context_word}' = {ppmi_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wBNxMGE7lb",
        "outputId": "015a38fc-baf9-4ed6-8001-185dd1b26cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI for 'executive' and 'said' = 4.76653639984982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.A.1. word = company and context word = of\n",
        "target_word = 'company'\n",
        "context_word = 'of'\n",
        "ppmi_value = calculate_ppmi_for_pair(target_word, context_word, word_freq, context_freq, pair_freq, total_count)\n",
        "print(f\"PPMI for '{target_word}' and '{context_word}' = {ppmi_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8iznPWdE7bU",
        "outputId": "0be27edb-49fc-48d7-be6d-1286cd5eb602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI for 'company' and 'of' = 4.3032070297415626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.A.1. word = company and context word = said\n",
        "target_word = 'company'\n",
        "context_word = 'said'\n",
        "ppmi_value = calculate_ppmi_for_pair(target_word, context_word, word_freq, context_freq, pair_freq, total_count)\n",
        "print(f\"PPMI for '{target_word}' and '{context_word}' = {ppmi_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmLMqn5IE7Nw",
        "outputId": "418bbcb7-3d83-41d1-befd-e25be8a576ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI for 'company' and 'said' = 4.448866737439181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.B : context words given as [said, of, president]. Calculate Cosine Similarity of words 1)sales and purchase 2) executive and company 3)executive and sales"
      ],
      "metadata": {
        "id": "kBcqicuE5FRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "# Function to calculate PPMI vectors for a given word and a set of context words\n",
        "def calculate_ppmi_vectors(words, contexts):\n",
        "    word_freq, context_freq, pair_freq, total_count = calculate_ppmi(tokenized_sentences)\n",
        "    # Step 4: Calculate PPMI vectors for each word\n",
        "    ppmi_vectors = {}\n",
        "    for word in words:\n",
        "        ppmi_vector = []\n",
        "        p_word = word_freq[word] / total_count\n",
        "        for context_word in contexts:\n",
        "            p_context = context_freq[context_word] / total_count\n",
        "            p_pair = pair_freq[(word, context_word)] / total_count\n",
        "\n",
        "            # Calculate PMI\n",
        "            if p_pair > 0 and p_word > 0 and p_context > 0:\n",
        "                pmi = math.log2(p_pair / (p_word * p_context))\n",
        "                ppmi = max(0, pmi)  # Convert PMI to PPMI\n",
        "            else:\n",
        "                ppmi = 0\n",
        "            ppmi_vector.append(ppmi)\n",
        "        ppmi_vectors[word] = ppmi_vector\n",
        "\n",
        "    return ppmi_vectors\n",
        "\n",
        "# Example usage\n",
        "words = ['sales', 'purchase', 'executive', 'company']\n",
        "contexts = ['said', 'of', 'president']\n",
        "\n",
        "# Calculate PPMI vectors\n",
        "ppmi_vectors = calculate_ppmi_vectors(words, contexts)\n",
        "\n",
        "# (1) Cosine similarity for 'sales' and 'purchase'\n",
        "cosine_sales_purchase = cosine_similarity(ppmi_vectors['sales'], ppmi_vectors['purchase'])\n",
        "print(f\"Cosine similarity (sales, purchase): {round(cosine_sales_purchase, 2)}\")\n",
        "\n",
        "# (2) Cosine similarity for 'executive' and 'company'\n",
        "cosine_executive_company = cosine_similarity(ppmi_vectors['executive'], ppmi_vectors['company'])\n",
        "print(f\"Cosine similarity (executive, purchase): {round(cosine_executive_company, 2)}\")\n",
        "\n",
        "# (3) Cosine similarity for 'executive' and 'sales'\n",
        "cosine_executive_sales = cosine_similarity(ppmi_vectors['executive'], ppmi_vectors['sales'])\n",
        "print(f\"Cosine similarity (executive, sales): {round(cosine_executive_sales, 2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L93uP2-58YI",
        "outputId": "c88fee98-516b-438c-e647-60a8bddde946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (sales, purchase): 0.96\n",
            "Cosine similarity (executive, purchase): 0.97\n",
            "Cosine similarity (executive, sales): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nt24AjkelSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Word Vectors using Glove embedding"
      ],
      "metadata": {
        "id": "Ajq4U0sYYBI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
        "!unzip glove.42B.300d.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-fzAb6-rh0F",
        "outputId": "ed859e8e-a74e-4062-cc9b-1444ba73e99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-25 19:00:54--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2024-09-25 19:00:54--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2024-09-25 19:00:54--  https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  5.00MB/s    in 5m 52s  \n",
            "\n",
            "2024-09-25 19:06:47 (5.08 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n",
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3.A: Find most similar words using glove embedding 1) business 2)school 3) state"
      ],
      "metadata": {
        "id": "k9aKqWxtxKo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the GloVe vectors into a dictionary\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings_dict = {}\n",
        "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "glove_file = 'glove.42B.300d.txt'  # Update with your file path\n",
        "embeddings_dict = load_glove_embeddings(glove_file)\n",
        "\n",
        "# Retrieve the word embedding for \"state\"\n",
        "business_vector = embeddings_dict[\"business\"]\n",
        "\n",
        "# Find the most similar word to \"state\" excluding \"state\" and \"states\"\n",
        "most_similar_word = None\n",
        "max_similarity = -1\n",
        "\n",
        "for word, vector in embeddings_dict.items():\n",
        "    if word in [\"business\", \"businesss\"]:\n",
        "        continue\n",
        "    similarity = cosine_similarity(business_vector, vector)\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        most_similar_word = word\n",
        "\n",
        "print(f\"The most similar word to 'business' is: {most_similar_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oknN4hnKYGVc",
        "outputId": "e0e515e1-1fbf-43a2-a1a3-0ed8fd41f8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar word to 'business' is: businesses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "school_vector = embeddings_dict[\"school\"]\n",
        "\n",
        "# Find the most similar word to \"business\" excluding \"business\" and \"businesss\"\n",
        "most_similar_word = None\n",
        "max_similarity = -1\n",
        "\n",
        "for word, vector in embeddings_dict.items():\n",
        "    if word in [\"school\", \"schools\"]:\n",
        "        continue\n",
        "    similarity = cosine_similarity(school_vector, vector)\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        most_similar_word = word\n",
        "\n",
        "print(f\"The most similar word to 'school' is: {most_similar_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4WXiKdKbMoS",
        "outputId": "2d9587dc-dee5-49f9-cafe-f62623d5e51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar word to 'school' is: elementary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_vector = embeddings_dict[\"state\"]\n",
        "\n",
        "most_similar_word = None\n",
        "max_similarity = -1\n",
        "\n",
        "for word, vector in embeddings_dict.items():\n",
        "    if word in [\"state\", \"states\"]:\n",
        "        continue\n",
        "    similarity = cosine_similarity(state_vector, vector)\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        most_similar_word = word\n",
        "\n",
        "print(f\"The most similar word to 'state' is: {most_similar_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JklinYsZImu8",
        "outputId": "18479378-8ce8-4708-ed59-b2a61e26195e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar word to 'state' is: government\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3.B: Find analogies 1)paris is to france as madrid is to ____"
      ],
      "metadata": {
        "id": "RD6tuCfWxdHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paris_vector = embeddings_dict[\"paris\"]\n",
        "france_vector = embeddings_dict[\"france\"]\n",
        "madrid_vector = embeddings_dict[\"madrid\"]\n",
        "analogy_vector = madrid_vector + france_vector - paris_vector\n",
        "most_similar_word = None\n",
        "max_similarity = -1\n",
        "\n",
        "for word, vector in embeddings_dict.items():\n",
        "    if word in [\"paris\", \"france\", \"madrid\"]:\n",
        "        continue\n",
        "    similarity = cosine_similarity(analogy_vector, vector)\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        most_similar_word = word\n",
        "\n",
        "print(f\"The word completing the analogy 'Paris is to France as Madrid is to X' is: {most_similar_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vunTwn6tJr5A",
        "outputId": "e6f04bf9-caaa-44e5-f8d5-2bbe44e1c06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word completing the analogy 'Paris is to France as Madrid is to X' is: spain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.lm import MLE, Laplace, StupidBackoff\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "\n",
        "def tokenize_sentences(sentences):\n",
        "    return [wordpunct_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "def get_context(tokens, index, window_size=8):\n",
        "    left_context = tokens[max(0, index - window_size):index]\n",
        "    right_context = tokens[index + 1:index + window_size + 1]\n",
        "\n",
        "    left_context = ['NIL'] * (window_size - len(left_context)) + left_context\n",
        "    right_context = right_context + ['NIL'] * (window_size - len(right_context))\n",
        "\n",
        "    return left_context + right_context\n",
        "\n",
        "def compute_co_occurrence(tokenized_sentences, window_size=8):\n",
        "    co_occurrence = defaultdict(Counter)\n",
        "    word_counts = Counter()\n",
        "    for sentence in tokenized_sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            context = get_context(sentence, i, window_size)\n",
        "            co_occurrence[word].update(context)\n",
        "            word_counts[word] += 1\n",
        "    # print(co_occurrence)\n",
        "    return co_occurrence, word_counts\n",
        "\n",
        "def compute_ppmi(word, context_word, co_occurrence, word_counts, total_words):\n",
        "    p_word = word_counts[word] / total_words\n",
        "    p_context = word_counts[context_word] / total_words\n",
        "    p_word_context = co_occurrence[word][context_word] / total_words\n",
        "\n",
        "\n",
        "    if p_word_context == 0:\n",
        "        return 0\n",
        "\n",
        "    pmi = math.log2(p_word_context / (p_word * p_context))\n",
        "    return max(0, pmi)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    tokenized_sentences = tokenize_sentences(corpus)\n",
        "\n",
        "    co_occurrence, word_counts = compute_co_occurrence(tokenized_sentences)\n",
        "    total_words = sum(word_counts.values())\n",
        "\n",
        "    word_context_pairs = [\n",
        "        (\"executive\", \"president\"),\n",
        "        (\"executive\", \"said\"),\n",
        "        (\"company\", \"said\"),\n",
        "        (\"company\", \"of\")\n",
        "    ]\n",
        "\n",
        "    for word, context_word in word_context_pairs:\n",
        "        ppmi = compute_ppmi(word, context_word, co_occurrence, word_counts, total_words)\n",
        "        print(f\"PPMI({word}, {context_word}) = {ppmi:.4f}\")"
      ],
      "metadata": {
        "id": "B9P_z0BnTO_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dae462d-6667-41f2-e67d-1cdc830e31be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI(executive, president) = 6.3745\n",
            "PPMI(executive, said) = 4.5779\n",
            "PPMI(company, said) = 4.2603\n",
            "PPMI(company, of) = 4.1597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AE-B1Q62XjHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}